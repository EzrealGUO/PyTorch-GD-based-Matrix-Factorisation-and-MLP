{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6fe922a",
   "metadata": {},
   "source": [
    "# Part 1: Forward Mode Automatic Differentiation\n",
    "\n",
    "Forward mode AD can simply be implemented by defining a class to represent [dual numbers](https://en.wikipedia.org/wiki/Dual_number) which hold the value and its derivative. The following skeleton defines a dual number and implements multiplication. \n",
    "\n",
    "__Tasks:__\n",
    "\n",
    "- Addition (`__add__`) is incomplete - can you finish it? \n",
    "- Can you also implement division (`__truediv__`), subtraction (`__sub__`) and power (`__pow__`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be76a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DualNumber:\n",
    "    def __init__(self, value, dvalue):\n",
    "        self.value = value\n",
    "        self.dvalue = dvalue\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.value) + \" + \" + str(self.dvalue) + \"ε\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return DualNumber(self.value + other.value,self.dvalue + other.dvalue)\n",
    "\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return DualNumber(self.value - other.value,self.dvalue - other.dvalue)\n",
    "\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return DualNumber(self.value * other.value,self.dvalue * other.value + other.dvalue * self.value)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return DualNumber(self.value / other.value,(self.dvalue * other.value - other.dvalue * self.value) / other.value ** 2)\n",
    "        \n",
    "    def __pow__(self, other):\n",
    "        return DualNumber(self.value ** other.value, other.value * self.value ** (other.value-1) * other.dvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1cdcd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 0ε\n",
      "1\n",
      "0\n",
      "4.7 + 2ε\n",
      "2.1 + 4.7ε\n",
      "2.579425538604203 + 5.577582561890373ε\n",
      "2 + 0ε\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "print(DualNumber(1,0)) \n",
    "print(DualNumber(1,0).value) \n",
    "print(DualNumber(1,0).dvalue) \n",
    "\n",
    "x = DualNumber(0.5,1)\n",
    "y = DualNumber(4.2,1)\n",
    "z = x.__add__(y)\n",
    "print(z)\n",
    "z = x.__mul__(y)\n",
    "print(z)\n",
    "z = x.__mul__(y) + sin(x)\n",
    "print(z)\n",
    "print(DualNumber(1,0) + DualNumber(1,0))\n",
    "# print(DualNumber(1,0) / DualNumber(1,0))\n",
    "# print(DualNumber(1,0) - DualNumber(1,0))\n",
    "# print(DualNumber(1,0) ** DualNumber(1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2292e80e",
   "metadata": {},
   "source": [
    "## Implementing math functions\n",
    "\n",
    "We also need to implement some core math functions. Here's the sine function for a dual number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0bcbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(x):\n",
    "    return DualNumber(math.sin(x.value), math.cos(x.value)*x.dvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8bf85",
   "metadata": {},
   "source": [
    "__Task:__ can you implement the _cosine_ (`cos`), _tangent_ (`tan`), and _exponential_ (`exp`) functions in the code block below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f51a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement additional math functions on dual numbers\n",
    "\n",
    "def cos(x):\n",
    "    # YOUR CODE HERE\n",
    "    return DualNumber(math.cos(x.value), -math.sin(x.value)*x.dvalue)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def tan(x):\n",
    "    # YOUR CODE HERE\n",
    "    return DualNumber(math.tan(x.value), x.dvalue/math.cos(x.value)**2)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def exp(x):\n",
    "    # YOUR CODE HERE\n",
    "    return DualNumber(math.exp(x.value), math.exp(x.value)*x.dvalue)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "166fde7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 + -0.0ε\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "assert cos(DualNumber(0,0)).value == 1\n",
    "assert tan(DualNumber(0,0)).value == 0\n",
    "assert exp(DualNumber(0,0)).value == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2977a304",
   "metadata": {},
   "source": [
    "## Time to try it out\n",
    "\n",
    "We're now in a position to try our implementation.\n",
    "\n",
    "__Task:__ \n",
    "\n",
    "- Try running the following code to compute the value of the function $z=x\\cdot y+sin(x)$ given $x=0.5$ and $y=4.2$, together with the derivative $\\partial z/\\partial x$ at that point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3534fef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.579425538604203 + 5.077582561890373ε\n",
      "2.579425538604203\n",
      "5.077582561890373\n"
     ]
    }
   ],
   "source": [
    "x = DualNumber(0.5,1)\n",
    "y = DualNumber(4.2,0)\n",
    "z = x.__mul__(y) + sin(x)\n",
    "print(z)\n",
    "print(z.value)\n",
    "print(z.dvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f663f5",
   "metadata": {},
   "source": [
    "__Task__: Differentiate the above function with respect to $x$ and write the symbolic derivatives in the following box. Verify the result computed above is correct by plugging-in the values into your symbolic gradient expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4fbc67d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.077582561890373\n"
     ]
    }
   ],
   "source": [
    "dzdx = y.value + math.cos(x.value)\n",
    "print(dzdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670ee44",
   "metadata": {},
   "source": [
    "__Task:__ Now use the code block below to compute the derivative $\\partial z/\\partial y$ of the above expression (at the same point $x=0.5, y=4.2$ as above) and store the derivative in the variable `dzdy` (just the derivative, not the Dual Number). Verify by hand that the result is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5437d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "x = DualNumber(0.5,0)\n",
    "y = DualNumber(4.2,1)\n",
    "dzdy1 = x.value\n",
    "print(dzdy1)\n",
    "z = x.__mul__(y) + sin(x)\n",
    "dzdy2 = z.dvalue\n",
    "print(dzdy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d56bb",
   "metadata": {},
   "source": [
    "# Part 2: Reverse Mode Automatic Differentiation\n",
    "\n",
    "Dynamic Reverse mode AD can be implemented by declaring a class to represent a value and the child expressions that the value depends on. We've provided the implementation that was shown in the lecture slides as a basis below, but it's missing some parts that will make it useful.\n",
    "\n",
    "__Tasks:__\n",
    "\n",
    "- Addition (`__add__`) is incomplete - can you finish it? \n",
    "- Can you also implement division (`__truediv__`), subtraction (`__sub__`) and power (`__pow__`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dbdb0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Var:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "        self.grad_value = None\n",
    "\n",
    "    def grad(self):\n",
    "        if self.grad_value is None:\n",
    "            self.grad_value = sum(weight * var.grad()\n",
    "                                  for weight, var in self.children)\n",
    "        return self.grad_value\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        z = Var(self.value * other.value)\n",
    "        self.children.append((other.value, z))\n",
    "        other.children.append((self.value, z))\n",
    "        return z\n",
    "\n",
    "    def __add__(self, other):\n",
    "        z = Var(self.value+other.value)\n",
    "        self.children.append((1,z))\n",
    "        other.children.append((1,z))\n",
    "        return z\n",
    "        \n",
    "    def __truediv__(self, other):\n",
    "        z = Var(self.value / other.value)\n",
    "        self.children.append((1/other.value, z))\n",
    "        other.children.append((-self.value/other.value**2, z))\n",
    "        return z\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        z = Var(self.value - other.value)\n",
    "        self.children.append((1, z))\n",
    "        other.children.append((1, z))\n",
    "        return z\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        z = Var(self.value ** other.value)\n",
    "        self.children.append((other.value*(x**(other.value-1)), z))\n",
    "        other.children.append((self.value ** other.value*math.log(self.value), z))\n",
    "        return z\n",
    "\n",
    "#     raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0fb3b4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[]\n",
      "<bound method Var.grad of <__main__.Var object at 0x000002E48471D430>>\n",
      "4.2\n",
      "[]\n",
      "<bound method Var.grad of <__main__.Var object at 0x000002E48471DC10>>\n",
      "4.7\n",
      "4.7\n",
      "<bound method Var.grad of <__main__.Var object at 0x000002E484813F40>>\n"
     ]
    }
   ],
   "source": [
    "x = Var(0.5)\n",
    "print(x.value)\n",
    "print(x.children)\n",
    "print(x.grad)\n",
    "y = Var(4.2)\n",
    "print(y.value)\n",
    "print(y.children)\n",
    "print(y.grad)\n",
    "z = x.__add__(y)\n",
    "print(z)\n",
    "print(z.value)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5575037",
   "metadata": {},
   "source": [
    "## Implementing math functions\n",
    "\n",
    "Just like when we were looking at Forward Mode AD, we also need to implement some core math functions. Here's the sine function for a `Var`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b3eeabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(x):\n",
    "    z = Var(math.sin(x.value))\n",
    "    x.children.append((math.cos(x.value), z))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a9dcb",
   "metadata": {},
   "source": [
    "__Task:__ can you implement the _cosine_ (`cos`), _tangent_ (`tan`), and _exponential_ (`exp`) functions in the code block below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a3d02a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement additional math functions on dual numbers\n",
    "\n",
    "def cos(x):\n",
    "    z = Var(math.cos(x.value))\n",
    "    x.children.append((-math.sin(x.value), z))\n",
    "    return z\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def tan(x):\n",
    "    z = Var(math.tan(x.value))\n",
    "    x.children.append((1/(math.cos(x.value))**2, z))\n",
    "    return z\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def exp(x):\n",
    "    z = Var(math.exp(x.value))\n",
    "    x.children.append((math.exp(x.value), z))\n",
    "    return z\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6490170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "assert cos(Var(0)).value == 1\n",
    "assert tan(Var(0)).value == 0\n",
    "assert exp(Var(0)).value == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ef90d",
   "metadata": {},
   "source": [
    "## Time to try it out\n",
    "\n",
    "We're now in a position to try our implementation.\n",
    "\n",
    "__Tasks:__ \n",
    "\n",
    "- Try running the following code to compute the value of the function $z=x\\cdot y+sin(x)$ given $x=0.5$ and $y=4.2$, together with the derivative $\\partial z/\\partial x$ at that point. \n",
    "- Verify that the result is correct by hand-differentiating the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "59c2d77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: 2.579425538604203\n",
      "∂z/∂x: 5.077582561890373\n"
     ]
    }
   ],
   "source": [
    "x = Var(0.5)\n",
    "y = Var(4.2)\n",
    "z = x * y + sin(x)\n",
    "print('z:', z)\n",
    "\n",
    "z.grad_value = 1.0 #Note that we have to 'seed' the gradient of z to 1 (e.g. ∂z/∂z=1) before computing grads\n",
    "print('∂z/∂x:',x.grad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "61fbc553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: 2.579425538604203\n",
      "∂z/∂x: 5.077582561890373\n"
     ]
    }
   ],
   "source": [
    "x = Var(0.5)\n",
    "y = Var(4.2)\n",
    "z = x.__mul__(y) + sin(x)\n",
    "print('z:', z)\n",
    "\n",
    "z.grad_value = 1.0 #Note that we have to 'seed' the gradient of z to 1 (e.g. ∂z/∂z=1) before computing grads\n",
    "print('∂z/∂x:',x.grad())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c9f42",
   "metadata": {},
   "source": [
    "__Task:__ Now use the code block below to compute the derivative $\\partial z/\\partial y$ of the above expression (at the same point $x=0.5, y=4.2$ as above). Store the resultant gradient in the variable `dzdy`. Verify by hand that the result is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "622cb6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂z/∂y: 0.5\n"
     ]
    }
   ],
   "source": [
    "x = Var(0.5)\n",
    "y = Var(4.2)\n",
    "z = y.__mul__(x) + sin(x)\n",
    "z.grad_value = 1.0\n",
    "dzdy = y.grad()\n",
    "print('∂z/∂y:', dzdy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9fe9fb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂z/∂y: 0.5\n"
     ]
    }
   ],
   "source": [
    "x = Var(0.5)\n",
    "y = Var(4.2)\n",
    "z = y*x + sin(x)\n",
    "z.grad_value = 1.0\n",
    "dzdy = y.grad()\n",
    "print('∂z/∂y:', dzdy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85fc566",
   "metadata": {},
   "source": [
    "## Differentiating Algorithms\n",
    "\n",
    "Now, let's look at doing something wacky: differentiate an algorithm. For this example, we'll use an algorithm that is in a sense static (in this particular case the upper limit of the for loop is predetermined). However, it is not difficult to see that AD is much more general, and could even be applied to stochastic algorithms (say if we replaced the upper limit of the loop below with `Math.floor(Math.random() * 10)` for example).\n",
    "\n",
    "__Task:__ Consider the following algorithm and in the box below it manually compute the value of $z$ and the gradient $\\partial z/\\partial x$ at the end of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "67c6fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Var(0.5)\n",
    "z = Var(1)\n",
    "for i in range(0,2):\n",
    "    z = (z + Var(i)) * x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b9293b",
   "metadata": {},
   "source": [
    "z1 = (z0+Var(0)) * 0.5 * 0.5 = (1+0) * 0.25 = 0.25\n",
    "\n",
    "z2 = (z1+Var(1)) * 0.5 * 0.5 = (0.25+1) * 0.25 = 0.3125\n",
    "\n",
    "__Task__: Now use the code block below to print out the gradient computed by our reverse AD by storing the result in a variable called `grad`. Does it match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8f844785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3125\n",
      "1.5\n"
     ]
    }
   ],
   "source": [
    "print(z.value)\n",
    "z.grad_value = 1.0\n",
    "print(x.grad())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798dfad",
   "metadata": {},
   "source": [
    "# Part 3: Reverse Mode Automatic Differentiation with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e020f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this code block to install dependencies when running on colab\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    from os.path import exists\n",
    "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd9b009",
   "metadata": {},
   "source": [
    "PyTorch implements Dynamic Reverse Mode Automatic Differentiation, much like we did in the previous exercise. There is one really major difference in what PyTorch provides over our simple example: it works directly with matrices (`Tensor`s) rather than with scalars (although obviously a matrix can represent a scalar).\n",
    "\n",
    "In this tutorial, we'll explore PyTorch's AD implementation. Note that we're using the API of PyTorch 0.4 or later which simplifies use of AD (previous versions required wrapping all `Tensor`s that you wanted to compute gradients of in `Variable` objects; PyTorch 0.4 removes the need to do this and allows `Tensor`s themselves to track gradients).\n",
    "\n",
    "We'll start with the simple example we tried earlier in the code block below:\n",
    "\n",
    "__Task:__ Run the following code and verify the solution is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7b78d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = 2.57942533493042\n",
      "dz/dx = 5.077582359313965\n",
      "dz/dy = 0.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# set up the problem\n",
    "x = torch.tensor(0.5, requires_grad=True)\n",
    "y = torch.tensor(4.2, requires_grad=True)\n",
    "z = x * y + torch.sin(x)\n",
    "\n",
    "print(\"z = \" + str(z.item()))\n",
    "\n",
    "z.backward() # this goes through the computation graph and accumulates the gradients in the cached .grad attributes\n",
    "print(\"dz/dx = \" + str(x.grad.item()))\n",
    "print(\"dz/dy = \" + str(y.grad.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285185e",
   "metadata": {},
   "source": [
    "As with our own AD implementation, PyTorch lets us differentiate through an algorithm.\n",
    "\n",
    "__Task__: Use the block below to compute the gradient $\\partial z/\\partial x$ of the following pseudocode algorithm and store the result in the `dzdx` variable:\n",
    "\n",
    "    x = 0.5\n",
    "    z = 1\n",
    "    i = 0\n",
    "    while i<2:\n",
    "        z = (z + i) * x * x\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f72d67c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(0.5, requires_grad=True)\n",
    "z = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "for i in range(0,2):\n",
    "    z = (z + i) * x * x\n",
    "    \n",
    "z.backward()\n",
    "\n",
    "dzdx = x.grad.item()\n",
    "\n",
    "print(dzdx)\n",
    "\n",
    "# # YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "992d10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dzdx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245dc578",
   "metadata": {},
   "source": [
    "## PyTorch limitations: in-place operations and aliasing\n",
    "\n",
    "PyTorch will throw an error at runtime if you try to differentiate through an in-place operation on a tensor. \n",
    "\n",
    "__Task__: Run the following code to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b224422a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of TanhBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-637e22e4c834>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# inplace addition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of TanhBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = x.tanh()\n",
    "y.add_(3) # inplace addition\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff70a08",
   "metadata": {},
   "source": [
    "Aliasing is also something that can't be differentiated through and will result in a slightly more cryptic error.\n",
    "\n",
    "__Task__: Run the following code to see this in action. If you don't understand what this code does add some `print` statements to show the values of `x` and `y` at various points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a5482fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a view of a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-93391ce06615>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a view of a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4], requires_grad=True, dtype=torch.float)\n",
    "y = x[:1]\n",
    "print(y)\n",
    "y.add_(3)\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63185479",
   "metadata": {},
   "source": [
    "## Dealing with multiple outputs\n",
    "\n",
    "PyTorch can deal with the case where there are multiple output variables if we can formulate the expression in terms of tensor operations. Consider the example from the presentation for example:\n",
    "\n",
    "$$\\begin{cases}\n",
    "     z = 2x + \\sin x\\\\\n",
    "     v = 4x + \\cos x\n",
    "\\end{cases}$$\n",
    "\n",
    "We could formulate this as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}z \\\\ v\\end{bmatrix} = \\begin{bmatrix}2 \\\\ 4\\end{bmatrix} \\odot \\bar{x} + \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} \\odot \\sin\\bar x + \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} \\odot \\cos\\bar x\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\bar x = \\begin{bmatrix}x \\\\ x\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and $\\odot$ represents the Hadamard or element-wise product. This is demonstrated using PyTorch in the following code block.\n",
    "\n",
    "__Task:__ run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d325bb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5403],\n",
      "        [3.1585]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.0],[1.0]], requires_grad=True)\n",
    "\n",
    "zv = ( torch.tensor([[2.0],[4.0]]) * x +\n",
    "         torch.tensor([[1.0], [0.0]]) * torch.sin(x) +\n",
    "         torch.tensor([[0.0], [1.0]]) * torch.cos(x) )\n",
    "        \n",
    "zv.backward(torch.tensor([[1.0],[1.0]])) # Note as we have \"multiple outputs\" we must pass in a tensor of weights of the correct size\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b02ea",
   "metadata": {},
   "source": [
    "__Task:__ Use the following box to write down the derivative of the expression for $\\begin{bmatrix}z \\\\ v\\end{bmatrix}$ and verify the gradients $\\partial z/\\partial x$ and $\\partial v/\\partial x$ are correct for $x=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc888f",
   "metadata": {},
   "source": [
    "$\\partial z/\\partial x$ = 2 + cos(x)\n",
    "\n",
    "$\\partial v/\\partial x$ = 4 - sin(x)\n",
    "\n",
    "when x = 1,\n",
    "\n",
    "$\\partial z/\\partial x$ = 2 + cos(x) = 2 + cos(1) = 2.5\n",
    "\n",
    "$\\partial v/\\partial x$ = 4 - sin(x) = 4 - sin(1) = 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c0a8e",
   "metadata": {},
   "source": [
    "## Gradient descent & gradients with respect to a vector\n",
    "Let's put everything together and using automatically computed gradients to find the minima of a function by taking steps down the gradient from an initial position. Rather than explicitly creating each input variable as a scalar as in the previous examples, we'll use a vector instead (so our gradients will be with respect to each element of the vector).\n",
    "\n",
    "__Task:__ work through the following example to see how taking gradients with respect to a vector works & how simple gradient descent optimisation can be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "647ad0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1.6000],\n",
      "        [0.8000],\n",
      "        [8.0000]], requires_grad=True), 67.19999694824219)\n",
      "(tensor([[0.1718],\n",
      "        [0.0859],\n",
      "        [0.8590]], requires_grad=True), 0.7747630476951599)\n",
      "(tensor([[0.0184],\n",
      "        [0.0092],\n",
      "        [0.0922]], requires_grad=True), 0.008932411670684814)\n",
      "(tensor([[0.0020],\n",
      "        [0.0010],\n",
      "        [0.0099]], requires_grad=True), 0.00010298370034433901)\n",
      "(tensor([[0.0002],\n",
      "        [0.0001],\n",
      "        [0.0011]], requires_grad=True), 1.1873213452417986e-06)\n",
      "(tensor([[2.2836e-05],\n",
      "        [1.1418e-05],\n",
      "        [1.1418e-04]], requires_grad=True), 1.3688886468798955e-08)\n",
      "(tensor([[2.4520e-06],\n",
      "        [1.2260e-06],\n",
      "        [1.2260e-05]], requires_grad=True), 1.5782215811999123e-10)\n",
      "(tensor([[2.6328e-07],\n",
      "        [1.3164e-07],\n",
      "        [1.3164e-06]], requires_grad=True), 1.8195657654207498e-12)\n",
      "(tensor([[2.8270e-08],\n",
      "        [1.4135e-08],\n",
      "        [1.4135e-07]], requires_grad=True), 2.0978168543115717e-14)\n",
      "(tensor([[3.0354e-09],\n",
      "        [1.5177e-09],\n",
      "        [1.5177e-08]], requires_grad=True), 2.418617877589929e-16)\n"
     ]
    }
   ],
   "source": [
    "# This is our starting vector\n",
    "initial=[[2.0], [1.0], [10.0]]\n",
    "\n",
    "# This is the function we will optimise (feel free to work out the analytic minima!)\n",
    "def function(x):\n",
    "    return x[0]**2 + x[1]**2 + x[2]**2\n",
    "\n",
    "x = torch.tensor(initial, requires_grad=True, dtype=torch.float)\n",
    "# print(x)\n",
    "# print(x.grad)\n",
    "# print(x.data)\n",
    "for i in range(0,100):\n",
    "    # manually dispose of the gradient (in reality it would be better to detach and zero it to reuse memory)\n",
    "    x.grad=None\n",
    "    # evaluate the function\n",
    "    J = function(x) \n",
    "    # auto-compute the gradients at the previously evaluated point x\n",
    "    J.backward()\n",
    "    # compute the update\n",
    "    x.data = x - x.grad*0.1 \n",
    "    \n",
    "    if i%10 == 0:\n",
    "        print((x, function(x).item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4b94c",
   "metadata": {},
   "source": [
    "__Task__: Answer the following question in the box below: Why must the update in the code above be assigned to `x.data` rather than `x`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e469793",
   "metadata": {},
   "source": [
    "## Differentiating through random operations\n",
    "\n",
    "We'll end with an example that will be important later in the course: differentiating with respect to the parameters of a random number generator.\n",
    "\n",
    "Assume that as some part of a differentiable program that we write we wish to incorporate a random element where we sample values, $z$ from a Normal distribution: $z \\sim \\mathcal{N}(\\mu,\\sigma^2)$. We want to learn the parameters of the generator $\\mu$ and $\\sigma^2$, but how can we do this? In a differentiable program setting we want to differentiate with respect to these parameters, but at first glance it isn't at all obvious what this means as the generator _just_ produces numbers which themselves don't have gradients.\n",
    "\n",
    "The answer is often called the _reparameterisation trick_: If we note that sampling a Normal distribution with a specfic mean and variance is equivalent to drawing numbers from a standard Normal distribution and scaling and shifting them: $z \\sim \\mathcal{N}(\\mu,\\sigma^2) \\equiv z \\sim \\mu + \\sigma\\mathcal{N}(0,1)\\equiv z = \\mu + \\sigma \\zeta\\, \\rm{where}\\, \\zeta\\sim\\mathcal{N}(0,1)$. With this reparameterisation the gradients with respect to the parameters are obvious.\n",
    "\n",
    "The following code block demonstrates this in practice; each of the gradients can be interpreted as how much an infintesimal change in $\\mu$ or $\\sigma$ would change $z$ if we could repeat the sampling operation again with the same value of `torch.randn(1)` being produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "10b1295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: 0.23901230096817017 \tdzdmu: 1.0 \tdzdsigma: -0.7609876990318298\n",
      "z: -0.2584792375564575 \tdzdmu: 1.0 \tdzdsigma: -1.2584792375564575\n",
      "z: 3.4020943641662598 \tdzdmu: 1.0 \tdzdsigma: 2.4020943641662598\n",
      "z: 2.4265389442443848 \tdzdmu: 1.0 \tdzdsigma: 1.4265388250350952\n",
      "z: 2.4836654663085938 \tdzdmu: 1.0 \tdzdsigma: 1.4836653470993042\n",
      "z: 0.2979324460029602 \tdzdmu: 1.0 \tdzdsigma: -0.7020675539970398\n",
      "z: -0.3962228298187256 \tdzdmu: 1.0 \tdzdsigma: -1.3962228298187256\n",
      "z: 0.05982029438018799 \tdzdmu: 1.0 \tdzdsigma: -0.940179705619812\n",
      "z: 0.9708083271980286 \tdzdmu: 1.0 \tdzdsigma: -0.029191698879003525\n",
      "z: 1.7798521518707275 \tdzdmu: 1.0 \tdzdsigma: 0.7798521518707275\n"
     ]
    }
   ],
   "source": [
    "mu = torch.tensor(1.0, requires_grad=True)\n",
    "sigma = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "for i in range(10):\n",
    "    mu.grad = None\n",
    "    sigma.grad = None\n",
    "    \n",
    "    z = mu + sigma * torch.randn(1)\n",
    "    z.backward()\n",
    "    print(\"z:\", z.item(), \"\\tdzdmu:\", mu.grad.item(), \"\\tdzdsigma:\", sigma.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850cc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
